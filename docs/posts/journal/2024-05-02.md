
[rec exploration](../../../rep-project/rec%20exploration.md)
### Rec


- [x] Create the rec engine pseudocode for:
	- [x] Matrix Cosine Similarity
	- [x] Content Based Recommender
- [x] Implement Basic Recommender Systems
- [ ] Experiment again the following
	- [ ] SVD Surprise
		- [ ] KNNBasic
		- [ ] matrix_favtorization.SVD
		- [ ] KNNWithMeans
	- [ ] Go by each while developing the pseudocode in parallel
	- [ ] Go By each while writing also the documentation of each (pseudo docs from research)
	- [ ] go and have the documentation improved at Jupyter Book.
- [ ] Implement Recommender from above (2)
- [ ] Second Phase
	- [ ] KNNWithZScore
	- [ ] Matrix Factorization.SVDpp
	- [ ] Matrix Factorization NMF
	- [ ] Slope One
- [ ] Implement General Tester
- [ ] Write down the Markdown documentation for the following.
	- [ ] (You could actually run this on a different like rogue docs.)
- [ ] Write down pseudocode for 


## Rec Engine Pseudocode 1


### Matrix Cosine Similarity

Now how to do this.. 
- [x] Matrix Cosine Similarity
- [x] Content Based Recommender

Since we already have them working. Let's just have the pseudocode based one what is working for them. 

This is what we are using:

![](img/Pasted%20image%2020240502112328.png)

Lets distinguish between:
- Setup
- recommend
- And how that can be extended into some sort of historical based rec.

```python
from sklearn.metrics.pairwise import cosine_similarity

  
# Setup
similarity_score = cosine_similarity(pt)
 
  

def recommend(book_name):
    index = np.where(pt.index==book_name)[0][0]
    similar_books = sorted(list(enumerate(similarity_score[index])),key=lambda x:x[1], reverse=True)[1:6]
    data = []

    for i in similar_books:

        item = []
        temp_df = books_df[books_df['Book-Title'] == pt.index[i[0]]]
        item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Title'].values))
        item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Author'].values))
        item.extend(list(temp_df.drop_duplicates('Book-Title')['Image-URL-M'].values))
        data.append(item)

    return data

  

def print_available_books():
    return list(final_ratings['Book-Title'].unique())

  

def search_name_like(name):
    return list(final_ratings[final_ratings['Book-Title'].str.contains(name)]['Book-Title'].unique())

def search_and_recommend(name):
    search_results = search_name_like(name)
    if len(search_results) == 0:
        return "No books found"
    return search_results, recommend(search_results[0])
    
```



- [x] Content Based Recommender


Final draft:

```python
  

from gensim.models import Word2Vec

import numpy as np

import random

  

class Word2VecRecommender(RecommendationAbstract):

    def __init__(self, products, embedding_size=100, window=5, min_count=1):

        super().__init__(products)

        # Train Word2Vec model

        self.model = Word2Vec(sentences=products, vector_size=embedding_size, window=window, min_count=min_count)

        # Build product embeddings

        self.product_embeddings = {product: self.model.wv[product] for product in self.products}

  

    def recommend_from_single(self, product, n=5):

        similar_products = self.model.wv.most_similar(product, topn=n)

        rec = [(product[0], product[1]) for product in similar_products]

        return rec

  

    def recommend_from_past(self, transactions, n=10):

        rec = []

        for transaction in transactions:

            rec.extend(self.recommend_from_single(transaction.product_id))

        # Sort recommendations based on similarity scores

        rec.sort(key=lambda x: x[1], reverse=True)

        return rec[:n]
```



But first we need to make sure that they are on the standard format. I like the keeping in place a `architecture`


- [x] Convert in Standard Format and save

- Also remove make sure that is good quality data and record the removals made.

- [ ] Implement Basic  Systemsx

ok, now that is saved look at the pseudocode and design its structure.

Ok, turns out I have to create this first:

```

```

![](img/Pasted%20image%2020240502130209.png)


So here the problem is that it requires (to be accepted, the cosine style per se)

- So lets start restoring the last . and also show that.
- And check for the following:

Is it possible to store the pivot table instead?

- Yes, but you actually want to store the cosine similarity score.
- Also because of the 

Add the following to be stored instead (the precalculated cosine similarity)

```python
import pickle


# Saving example
similarity_score = cosine_similarity(pt)
file_simscr = open('filesimscr', 'wb')
pickle.dump(similarity_score, file_simscr)
file_simscr.close()  # It's good practice to close the file after writing


# Loading example
file_simscr = open('filesimscr', 'rb')
loaded_similarity_score = pickle.load(file_simscr)
file_simscr.close()  # Close the file after reading

```


Now here the best attempt at storing the sim score etc.

```
# Implementation of the class using cosine Similarity.

  

class CosineSimilarityRecommender(RecommendationAbstract):

    strategy_name: str = "Cosine Similarity"

    slug_name: str = "cosine_similarity"

    version: str = "v1"

    details: str = "REQUIRES IMPLEMENTATION"

    link: str = "REQUIRES IMPLEMENTATION"

    supports_single_recommendation: bool = True

    supports_past_recommendation: bool = True

    def __init__(self, products, product_data):

        super().__init__(products, product_data)

        self.pt = []

        self.sim_score = None

    def train(self, transactions):

        self.pt = transactions.pivot_table(index="product_id", columns="user_id", values="rate")

        self.pt.fillna(0, inplace=True)

        self.sim_score = cosine_similarity(pt)

    def get_filename(self, filename_prepend):

        return  + self.product_data["unique_name"] + ".pik"

    def save(self):

        # Store self.pt

        filename = self.get_filename()

        file_simscr = open(filename, 'wb')

        pickle.dump(self.sim_score, file_simscr)

        file_simscr.close()

    def load(self):

        filename = self.get_filename()

        file_simscr = open(filename, 'rb')

        self.sim_score = pickle.load(file_simscr)

        file_simscr.close()

  

    def recommend_from_single(self, product_id, n=5):

        index = np.where(self.products == product_id)[0][0]

        similar_products = sorted(list(enumerate(self.similarity_matrix[index])), key=lambda x: x[1], reverse=True)[1:n+1]

        rec = [self.products[similar_product[0]] for similar_product in similar_products]

        return rec

  

    def recommend_from_past(self, transactions, n=10):

        rec = []

        for transaction in transactions:

            rec.extend(self.recommend_from_single(transaction['product_id']))

        random.shuffle(rec)

        return rec[:n]

```



Lets transfer some methods up:

```python
def get_filename(self):
	return self.slug_name + self.product_data["unique_name"] + ".pik"

def random_rec(self):
	return self.products[random.randint(0, len(self.products))]


```


```bash
KeyError Traceback (most recent call last) File [c:\github\explorations\recommendation-systems-exploration\venv\lib\site-packages\pandas\core\indexes\base.py:3805](file:///C:/github/explorations/recommendation-systems-exploration/venv/lib/site-packages/pandas/core/indexes/base.py:3805), in Index.get_loc(self, key) [3804](file:///C:/github/explorations/recommendation-systems-exploration/venv/lib/site-packages/pandas/core/indexes/base.py:3804) try: -> [3805](file:///C:/github/explorations/recommendation-systems-exploration/venv/lib/site-packages/pandas/core/indexes/base.py:3805) return self._engine.get_loc(casted_key) [3806](file:///C:/github/explorations/recommendation-systems-exploration/venv/lib/site-packages/pandas/core/indexes/base.py:3806) except KeyError as err: File index.pyx:167, in pandas._libs.index.IndexEngine.get_loc() File index.pyx:196, in pandas._libs.index.IndexEngine.get_loc() File pandas\\_libs\\hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item() File pandas\\_libs\\hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item() KeyError: 6212 The above exception was the direct cause of the following exception: KeyError Traceback (most recent call last) Cell In[64], [line 1](vscode-notebook-cell:?execution_count=64&line=1) ----> [1](vscode-notebook-cell:?execution_count=64&line=1) cosineRecommender.recommend_from_single(rand['id']) Cell In[58], [line 42](vscode-notebook-cell:?execution_count=58&line=42) [40](vscode-notebook-cell:?execution_count=58&line=40) index = np.where(self.products == product_id)[0][0]

...

[3815](file:///C:/github/explorations/recommendation-systems-exploration/venv/lib/site-packages/pandas/core/indexes/base.py:3815) # InvalidIndexError. Otherwise we fall through and re-raise [3816](file:///C:/github/explorations/recommendation-systems-exploration/venv/lib/site-packages/pandas/core/indexes/base.py:3816) # the TypeError. [3817](file:///C:/github/explorations/recommendation-systems-exploration/venv/lib/site-packages/pandas/core/indexes/base.py:3817) self._check_indexing_error(key)


```



```python

def recommend(book_name):
    index = np.where(pt.index==book_name)[0][0]
    similar_books = sorted(list(enumerate(similarity_score[index])),key=lambda x:x[1], reverse=True)[1:6]
    data = []

    for i in similar_books:

        item = []
        temp_df = books_df[books_df['Book-Title'] == pt.index[i[0]]]
        item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Title'].values))
        item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Author'].values))
        item.extend(list(temp_df.drop_duplicates('Book-Title')['Image-URL-M'].values))
        data.append(item)

    return data
```

![](img/Pasted%20image%2020240502155345.png)

I am not sure here what is the error. 

- I been stuck here for almost an hour. 
- My best guess is that it is because loading is not working properly
- There for lets just run the trainning
- Nevermind, so the index is received thats okay. Now the probelem gets here

```python
similar_books = sorted(list(enumerate(similarity_score[index])),key=lambda x:x[1], reverse=True)[1:6]

data = []

for i in similar_books:

	item = []

	temp_df = books_df[books_df['Book-Title'] == pt.index[i[0]]]
```




  
The error message `KeyError: 1415` indicates that there is an attempt to access a key in a data structure (likely a dictionary or pandas Series/DataFrame) that does not exist.

In this specific context, it seems like you're trying to access the key `1415` in an index or a dictionary, but that key does not exist in the structure you're trying to access. This could be happening in the `recommend_from_single` method of a `cosineRecommender` object.

Here are a few possible reasons for this error:

1. **Data Mismatch**: The key `1415` does not exist in the index or dictionary you're trying to access. It could be due to a typo or because the key is genuinely missing from the data.
    
2. **Incorrect Data Type**: The index or dictionary you're trying to access might have keys of a different data type than expected. For example, if you're expecting integer keys but the keys are strings, it can cause a `KeyError`.
    
3. **Data Corruption**: If the data has been manipulated or modified incorrectly before this point, it could lead to keys being missing or incorrectly formatted.
    

To diagnose this issue, you should trace back the code to where the `recommend_from_single` method is being called and ensure that the key being passed to it is valid and exists in the data structure being accessed. Additionally, you may want to examine the data structure itself to see if there are any discrepancies.


```python
def recommend_from_single(self, product_id, n=5):

	index = np.where(self.pt.index == product_id)

	print("index", index)

	index = np.where(self.pt.index == product_id)[0][0]

	print('Index accessed', index)

	similar_products = sorted(list(enumerate(self.sim_score[index])), key=lambda x: x[1], reverse=True)[1:n+1]

	rec = [self.products[similar_product[0]] for similar_product in similar_products]

	return rec
```


- Lets print the sim score and 


Ok, awesome this means that this part is working:

![](img/Pasted%20image%2020240502160640.png)


- Should I just start writing the lambda for receiving this?

![](img/Pasted%20image%2020240502160803.png)


```python  

def recommend_from_past(self, transactions, n=10):

	rec = []

	for transaction in transactions:

		rec.extend(self.recommend_from_single(transaction['product_id']))

	random.shuffle(rec)

	return rec[:n]
```


This line is wrong:

```
	rec = [self.products[similar_product[0]] for similar_product in similar_products]
```

Clearly products we don't have that.


Working sort of!

###  Cont Based Recommender

- [x] Lets try with the content rec.
- [ ] Create The class implementation correctly


![](img/Pasted%20image%2020240502164505.png)


**Setup**


You can appreciate that there is a model that is created before and after. using the `soup` strategy.

```python
  

# Preprocess text data
data['processed_soup'] = data['product_soup'].str.lower().str.translate(str.maketrans('', '', string.punctuation))

# Prepare data for Word2Vec model
sentences = [row.split() for row in data['processed_soup'].dropna()]
  

# Train Word2Vec model

model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

  

# Save the model for later use

model.save("../data/book_wordvec.model")
```



 SVD Surprise
- [ ] KNNBasic
- [ ] matrix_favtorization.SVD
- [ ] KNNWithMeans


### KNN Basic



## 3
### Looking at strange behavior:
![](img/Pasted%20image%2020240503111719.png)


**Control case**
![](img/Pasted%20image%2020240503112657.png)
```
SELECT t.hour, t.event_guid, t.day, t.month, t.year,t.hour_local, t.minute_local, t.tl10, e.app, e.integration_name, e.integration_name
FROM public.timeslot as t
LEFT JOIN public.event as e ON t.event_guid = e.guid 
WHERE DAY = 19 AND month = 4  AND hour_local = 18 AND minute_local<30 AND minute_local > 20 AND year=2024
ORDER BY t.id DESC;

```

This is correct:

![](img/Pasted%20image%2020240503112642.png)

But this is incorrect:

```sql
SELECT t.hour, t.event_guid, t.day, t.month, t.year,t.hour_local, t.minute_local, t.tl10, e.app, e.integration_name, e.integration_name
FROM public.timeslot as t
LEFT JOIN public.event as e ON t.event_guid = e.guid 
WHERE DAY = 19 AND month = 4  AND hour_local = 18 AND minute_local<20 AND minute_local > 10 AND year=2024
ORDER BY t.id DESC;

```
Sql Response:
![](img/Pasted%20image%2020240503112801.png)

![](img/Pasted%20image%2020240503112749.png)
![](img/Pasted%20image%2020240503115244.png)
Same error occurs:

![](img/Pasted%20image%2020240503115424.png)

```
SELECT t.hour, t.event_guid, t.day, t.month, t.year,t.hour_local, t.minute_local, t.tl10, e.app, e.integration_name, e.integration_name, e.user_id
FROM public.timeslot as t
LEFT JOIN public.event as e ON t.event_guid = e.guid 
WHERE DAY = 19 AND month = 4  AND hour_local = 13 AND minute_local<40 AND minute_local > 30 AND year=2024
ORDER BY t.id DESC;

```

![](img/Pasted%20image%2020240503115128.png)

And 
![](img/Pasted%20image%2020240503115342.png)
![](img/Pasted%20image%2020240503115401.png)


Lets 

### An first Approach to Gambit

- This is but a sample of the coop vs defect

Figuring out game theory strategy and mutation

First error:

![](img/Pasted%20image%2020240503142700.png)

Suddenly there is no population?

```
(venv) C:\github\explorations\genetic-algorithms-exploration\Hands-On-Genetic-Algorithms-with-Python\g-theory>python ecology-coop.py
gen     population      coop_pop        defect_pop
0       50              29              21
Offspring after tournamentSelection: []
Offspring after fitreproduceVarAnd: []
1       0               0               0
```

So the error is at selection


Trying to fix it lead to:

```
al2: {individual2}: {strategyInd2} - Fitness1: {fitnessInd1Inc} - Fitness2: {fitnessInd2Inc}")

        # # Increase the fitness of the individuals.

        # print(len(individual1.fitness.values), len(individual2.fitness.values))

        # print(individual1.fitness.values, individual2.fitness.values)

        individual1.fitness.values = tuple(fitnessInd1Inc)

        individual2.fitness.values = tuple(fitnessInd2Inc)
```


And now there is a complain of:

```
(0,) (1.0,)
Population before tournamentSelection:  50
Individuals at start : 50
Individual1: [0, 1, 0, 0, 0, 1, 1, 0, 0, 0]: COOPERATE - Individual2: [0, 1, 1, 1, 0, 0, 0, 0, 1, 0]: COOPERATE - Fitness1: 2 - Fitness2: 2
Traceback (most recent call last):
  File "C:\github\explorations\genetic-algorithms-exploration\Hands-On-Genetic-Algorithms-with-Python\g-theory\ecology-coop.py", line 85, in <module>   
    main()
  File "C:\github\explorations\genetic-algorithms-exploration\Hands-On-Genetic-Algorithms-with-Python\g-theory\ecology-coop.py", line 65, in main       
    population, logbook =  eaGambit(population, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION,
  File "C:\github\explorations\genetic-algorithms-exploration\Hands-On-Genetic-Algorithms-with-Python\g-theory\elitism.py", line 146, in eaGambit       
    offspring = selTournamentGambit(population)
  File "C:\github\explorations\genetic-algorithms-exploration\Hands-On-Genetic-Algorithms-with-Python\g-theory\elitism.py", line 69, in selTournamentGambit
    individual1.fitness.values = tuple(fitnessInd1Inc)
TypeError: 'int' object is not iterable
```


- Lets check how they are actually computed here at the start.

So I hijacked some code that returned me the following:

```
    invalid_ind = [ind for ind in population if not ind.fitness.valid]

    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)

    for ind, fit in zip(invalid_ind, fitnesses):

        print(f'ind: {ind}, fit: {fit}', type(fit) )

        ind.fitness.values = fit

        break

    return population, logbook
```


Got:

```
(venv) C:\github\explorations\genetic-algorithms-exploration\Hands-On-Genetic-Algorithms-with-Python\Chapter06>python 01-optimize-eggholder.py
ind: [142.77304162087296, -486.38898665198906], fit: (264.47494275897134,) <class 'tuple'>
(264.47494275897134,) (-1.0,)
Traceback (most recent call last):
```


Which is interesting because then we have a uple and duing basically the same here:

```
ind.fitness.values = fit
..

individual2.fitness.values = tuple(fitnessInd2Inc)

```



Awesome!

Here some notes to have:

- [x] Allow for gambit selection of the values.
- [x] Allow initial setup of different start population types.
- [x] Allow validating seeding or not.
- [x] Have precurated cases with some lore on them => at least on the report
- [ ] Have option to allow or not the *mating* (descendants).
	- What if the social ranking is determined by the food hunt*?
	- [ ] Create multiple mating structures
- [ ] Option to allow or not the mutation of random genes (random by offspring)
- [ ] Allow  mltiple


How do I create the individual value?
- Like the representation is okay, I am wondering how to make it so that it has a fitness param still.

Allow for different theorems as well. For example if both are aggressive, leaves the option of a random "after combat results: whereas"


For that you have to leave with the options to change such.



Rechecking: I might have the wrong understanding of selection:

![](img/Pasted%20image%2020240503175622.png)

Because are those the k, or times of the selection for the population or does it pair them? 
No, is getting the whole population. Things however should it find a random pair?


- I think I of got things kind of confused because of the population = sel.

In this case, I am guessing that is because there is but a very simple selection of tournaments each. and then pairs of random var and using those mutated values. While you actually want to create the offspring because of the varAnd there is where you should use the pairs* 


Since I am not selecting and giving priority at selection. I do howeer am selecting the offspring of the fitness (counts) then the multiplication should occur on the other page. Where it isbased as said of the eval function (that has an offspring count. )




- [x] Add encounter Eval Implementation
- [ ] Add Actual Eval implementation with `supported_reproduction_algorithm`

Add actual `selection` strategy

  
  

What seems to happen is that the recommender for

  

```python

# use closeness instead

data = Dataset.load_from_df(final_ratings[['User-ID', 'Book-Title', 'Book-Rating']], reader)

model = SVD()

model.fit(data.build_full_trainset())

product_name = "The Notebook"

model.get_neighbors(product_name, k=10)

  
  

sim_options = {

    "name": "cosine",

    "user_based": False,  # compute  similarities between items

}

algo = KNNBasic(sim_options=sim_optionssim_options = {

    "name": "cosine",

    "user_based": False,  # compute  similarities between items

}

algo = KNNBasic(sim_options=sim_options)

  

Get the sim options

```

Here the issues seems to be that sbd doenst have sim options?

```python
# use closeness instead

data = Dataset.load_from_df(final_ratings[['User-ID', 'Book-Title', 'Book-Rating']], reader)

model = SVD()

model.fit(data.build_full_trainset())

product_name = "The Notebook"

model.get_neighbors(product_name, k=10)
```


![](img/Pasted%20image%2020240508153235.png)
Documentation:

```
Return the `k` nearest neighbors of `iid`, which is the inner id of a user or an item, depending on the `user_based` field of `sim_options` (see [Similarity measure configuration](https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#similarity-measures-configuration)).

As the similarities are computed on the basis of a similarity measure, this method is only relevant for algorithms using a similarity measure, such as the [k-NN algorithms](https://surprise.readthedocs.io/en/stable/knn_inspired.html#pred-package-knn-inpired).
```


It appears that SVD doesnt have that support. Only knn.

```python
    with open(file_name, encoding="ISO-8859-1") as f:

        # Backup as csv.

        csv_file = "ml100k_items.csv"

        """

        movie id | movie title | release date | video release date |

        IMDb URL | unknown | Action | Adventure | Animation |

        Children's | Comedy | Crime | Documentary | Drama | Fantasy |

        Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |

        Thriller | War | Western |

        Convert into the following standarized format:

        | name           | type    | description                               |

        | -------------- | ------- | ----------------------------------------- |

        | id             | str     | Unique identifier of the product          |

        | product_title  | str     | Title of the product                      |

        | product_image  | str\nan | Image of the product                      |

        | product_price  | int\nan | Price of the product                      |

        | product_soup   | str     | All Aggregated Description of the product |

        | product_images | str\nan | List of images of the product             |

        | product_tags   | str\nan | List of tags of the product, sep by comma |

        """

        with io.open(csv_file, "w", encoding="ISO-8859-1") as out:

            out.write("id, product_title, product_image, product_price, product_soup, product_images, product_tags\n")

            for line in f:

                line = line.split("|")

                rid_to_name[line[0]] = line[1]

                name_to_rid[line[1]] = line[0]

                out.write(f"{line[0]}, {line[1]}, , , , , \n")

        for line in f:

            line = line.split("|")

            rid_to_name[line[0]] = line[1]

            name_to_rid[line[1]] = line[0]

        print("rid_to_name, name_to_rid")

        print(rid_to_name, name_to_rid)
```

### A save of SVD

```python
# Implementation of the class using cosine Similarity.

  

class MatrixFactorizationSVDRecommender(RecommendationAbstract):

    strategy_name: str = "Matrix Factorization SVD"

    slug_name: str = "matrix_factorization_svd"

    version: str = "v1"

    details: str = "REQUIRES IMPLEMENTATION"

    link: str = "REQUIRES IMPLEMENTATION"

    supports_single_recommendation: bool = True

    supports_past_recommendation: bool = True

    def __init__(self, products: pd.DataFrame, product_data: dict):

        super().__init__(products, product_data)

        self.products = products

        self.model = None

        # Get the product ids and store them.

        self.product_ids = self.products['id'].unique()

    def train(self, transactions, auto_save=True):

        model = SVD()

        reader = Reader(rating_scale=(1, 5))

        data = Dataset.load_from_df(transactions[['user_id', 'product_id', 'rate']], reader)

        model.fit(data.build_full_trainset())

        self.model = model

        # self.accuracy = accuracy.rmse(model.test(data.build_full_trainset().build_testset()), verbose=True)

        if auto_save:

            self.save()

    def get_filename(self):

        return "models/" + self.slug_name + self.product_data["unique_name"] + ".pik"

    def save(self):

        # Store self.pt

        filename = self.get_filename()

        model_file = open(filename, 'wb')

        pickle.dump(self.model)

        model_file.close()

    def load(self):

        filename = self.get_filename()

        model_file = open(filename, 'rb')

        self.model = pickle.load(model_file)

        model_file.close()

  

    def recommend_from_single(self, product_id: str, n=5) -> List[Tuple[dict, float]]:

        recommendation_list: List[tuple[dict, float]] = []

        filtered_products = self.products[self.products['id'] == product_id]

        for book_id in filtered_products:

            prediction = self.model.predict(product_id, book_id)

            recommendation_list.append((self.id_to_productDetail(book_id), prediction.est))

        # sort

        recommendation_list.sort(key=lambda x: x[1], reverse=True)

        return recommendation_list[:n]

  
  

    def recommend_from_past(self, transactions: List[str], n=10):

        rec: List[tuple[dict, float]] = []

        filtered_products = self.products[self.products['id'].isin(transactions)]

        # Sort by the confidence (second parameter of tuple)

        sorted_rec: List[tuple[dict, float]] = sorted(rec, key=lambda x: x[1], reverse=True)

        return sorted_rec[:n]
```


Get the recommendations

```python

randomProduct = engineRec.get_random_recommendation()[0]

pprint.pprint(randomProduct)

  

print('======== RECOMENDATIONS SINGLE CASE =========== ')

engineRec.recommend_from_single(randomProduct['id'])


```

```python
  

# ... Repetition.

print("=============  RECOMENDATIONS RECOMMENDATIONS  ============")

tansactions = ['0590353403', '0439139597']

  

"""

Harry Potter and the Sorcerer's Stone (Book 1)

"Harry Potter and the Goblet of Fire (Book 4)"

"""

  

rec = engineRec.recommend_from_past(tansactions)

pprint.pprint(rec)

```

Lets try with recommender:

```
get_neighbors

[56](vscode-notebook-cell:?execution_count=113&line=56) """ [57](vscode-notebook-cell:?execution_count=113&line=57) [58](vscode-notebook-cell:?execution_count=113&line=58) # Retrieve inner ids of the nearest neighbors of Toy Story. [59](vscode-notebook-cell:?execution_count=113&line=59) toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10) [60](vscode-notebook-cell:?execution_count=113&line=60) """ [61](vscode-notebook-cell:?execution_count=113&line=61) recommendation_list: List[tuple[dict, float]] = [] ---> [63](vscode-notebook-cell:?execution_count=113&line=63) neighbors = self.model.get_neighbors(self.product_ids.index(product_id), k=n) [65](vscode-notebook-cell:?execution_count=113&line=65) for neighbor in neighbors: [66](vscode-notebook-cell:?execution_count=113&line=66) product = self.products.iloc[neighbor] AttributeError: 'NoneType' object has no attribute 'get_neighbors'

```

Awesome, got it working:

```python
# Implementation of the class using cosine Similarity.

from surprise import KNNBasic

  

class BasicKNNRecommender(RecommendationAbstract):

    strategy_name: str = "Basic KNN"

    slug_name: str = "basic_knn"

    version: str = "v1"

    details: str = "REQUIRES IMPLEMENTATION"

    link: str = "REQUIRES IMPLEMENTATION"

    supports_single_recommendation: bool = True

    supports_past_recommendation: bool = True

    def __init__(self, products: pd.DataFrame, product_data: dict):

        super().__init__(products, product_data)

        self.products = products

        self.model = None

        # Get the product ids and store them.

        self.product_ids = self.products['id'].unique()

    def train(self, transactions, auto_save=True):

        sim_options = {"name": "pearson_baseline", "user_based": False}

        model = KNNBasic(sim_options=sim_options)

        reader = Reader(rating_scale=(1, 5))

        data = Dataset.load_from_df(transactions[['user_id', 'product_id', 'rate']], reader)

        model.fit(data.build_full_trainset())

        self.model = model

        # self.accuracy = accuracy.rmse(model.test(data.build_full_trainset().build_testset()), verbose=True)

        if auto_save:

            self.save()

    def get_filename(self):

        return "models/" + self.slug_name + self.product_data["unique_name"] + ".pik"

    def save(self):

        # Store self.pt

        filename = self.get_filename()

        model_file = open(filename, 'wb')

        pickle.dump(self.model, model_file)

        model_file.close()

    def load(self):

        filename = self.get_filename()

        model_file = open(filename, 'rb')

        self.model = pickle.load(model_file)

        model_file.close()

  

    def recommend_from_single(self, product_id: str, n=5) -> List[Tuple[dict, float]]:

        """

        # Retrieve inner ids of the nearest neighbors of Toy Story.

        toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)

        """

        recommendation_list: List[tuple[dict, float]] = []

        product_inner_id = self.model.trainset.to_inner_iid(product_id)

        neighbors = self.model.get_neighbors(product_inner_id, k=n)

        for neighbor in neighbors:

            product = self.products.iloc[neighbor]

            recommendation_list.append((product, 1.0))

        return recommendation_list[:n]

  
  

    def recommend_from_past(self, transactions: List[str], n=10):

        pass
```


Lets try now checking how to make it so that it has a recommendation from past:

- I was thinking about having 

```
  

engineRec = BasicKNNRecommender(productdf, product_data)
# engineRec.train(transactions=transactiondf, auto_save=True)
engineRec.load()
  
  

randomProduct = engineRec.get_random_recommendation()[0]
pprint.pprint(randomProduct)


print('======== RECOMENDATIONS SINGLE CASE =========== ')
engineRec.recommend_from_single(randomProduct['id'])


print("=============  RECOMENDATIONS RECOMMENDATIONS  ============")
tansactions = ['0590353403', '0439139597']
recomendations = engineRec.recommend_from_past(tansactions)
pprint.pprint(recomendations)
```

- Make it so tat the recommendations of the engine gets appended (default behavior) Then retrained for it.

- [ ] Search if there is already a support for it?
- [ ] Like adding things to the database and barely retraining?

For example here

The Reader class is used to parse a file containing ratings.

Such a file is assumed to specify only one rating per line, and each line needs to respect the following structure:

user ; item ; rating ;
where the order of the fields and the separator (here ‘;’) may be arbitrarily defined (see below). brackets indicate that the timestamp field is optional.

For each built-in dataset, Surprise also provides predefined readers which are useful if you want to use a custom dataset that has the same format as a built-in one (see the `name` parameter).

By having such a specific class just for readings. Does this makes sense? 

- Nope. I guess not such a support availble.

```
[[docs]](https://surprise.readthedocs.io/en/stable/algobase.html#surprise.prediction_algorithms.algo_base.AlgoBase.predict)    def predict(self, uid, iid, r_ui=None, clip=True, verbose=False):
        """Compute the rating prediction for given user and item.

        The ``predict`` method converts raw ids to inner ids and then calls the
        ``estimate`` method which is defined in every derived class. If the
        prediction is impossible (e.g. because the user and/or the item is
        unknown), the prediction is set according to
        :meth:`default_prediction()
        <surprise.prediction_algorithms.algo_base.AlgoBase.default_prediction>`.

        Args:
            uid: (Raw) id of the user. See :ref:`this note<raw_inner_note>`.
            iid: (Raw) id of the item. See :ref:`this note<raw_inner_note>`.
            r_ui(float): The true rating :math:`r_{ui}`. Optional, default is
                ``None``.
            clip(bool): Whether to clip the estimation into the rating scale.
                For example, if :math:`\\hat{r}_{ui}` is :math:`5.5` while the
                rating scale is :math:`[1, 5]`, then :math:`\\hat{r}_{ui}` is
                set to :math:`5`. Same goes if :math:`\\hat{r}_{ui} < 1`.
                Default is ``True``.
            verbose(bool): Whether to print details of the prediction.  Default
                is False.

        Returns:
            A :obj:`Prediction\
            <surprise.prediction_algorithms.predictions.Prediction>` object
            containing:

            - The (raw) user id ``uid``.
            - The (raw) item id ``iid``.
            - The true rating ``r_ui`` (:math:`r_{ui}`).
            - The estimated rating (:math:`\\hat{r}_{ui}`).
            - Some additional details about the prediction that might be useful
              for later analysis.
        """

        # Convert raw ids to inner ids
        try:
            iuid = self.trainset.to_inner_uid(uid)
        except ValueError:
            iuid = "UKN__" + str(uid)
        try:
            iiid = self.trainset.to_inner_iid(iid)
        except ValueError:
            iiid = "UKN__" + str(iid)

        details = {}
        try:
            est = self.estimate(iuid, iiid)

            # If the details dict was also returned
            if isinstance(est, tuple):
                est, details = est

            details["was_impossible"] = False

        except PredictionImpossible as e:
            est = self.default_prediction()
            details["was_impossible"] = True
            details["reason"] = str(e)

        # clip estimate into [lower_bound, higher_bound]
        if clip:
            lower_bound, higher_bound = self.trainset.rating_scale
            est = min(higher_bound, est)
            est = max(lower_bound, est)

        pred = Prediction(uid, iid, r_ui, est, details)

        if verbose:
            print(pred)

        return pred
```


## 9


### Adding Timestamps and Timestamp Locals to the processor


- [ ] Adding Timestamp Locals and Timestamps to the Timeslots event


What this means in terms of processing?

- Adding `end_times`
- Adding `start_times`.


Do we have an endtime or was that there only for calculations purposes?

Try to understand this one as well

![](img/Pasted%20image%2020240509095501.png)


### Re processing


**First attempt with Dev Database**
- Create the steps in the processing with timestamp and timestamp local
- Delete all Dev Database events
- Run the processing with the local.
- Now lets ensure fir


- [ ] Ensure first that you have a method to quickly populate dev.
- [ ] Ensure first you have method to quickly populate stage
- [ ] Delete All Stage
- [ ] Populate all stage


Wait what? this is clearly wrong:
- This fetching at the moment where I am fetching from details instead of trying to load from the  `s3_key` file is wrong.
- Am I perhaps loading things there? no.

- [x]  Check if the S3 file is created at lambda or not.
-  [x] Check if the s3 file in the database makes sense also

![](img/Pasted%20image%2020240509150839.png)

```
s3://dev-stage-in/0238bfa6-2a89-42fe-8a67-fde4c80391ea/SFFLOW/4c374db5-8e37-428e-b988-92ce9879165a/2024-03-05/2d1b2c20-6a4a-4ce3-aec1-459afe353d70.json
```

- Here the same.
- begs the question, am I using s3 for anything?

You can see here that the details should be extracted from the s3 file instead (best practice)

![](img/Pasted%20image%2020240509151048.png)

- [ ] S3Key | works where the sqs.send_message(...)


```python

response = sqs.send_message(
            QueueUrl=url,
            DelaySeconds=10,
            MessageAttributes={
                'guid': {
                    'DataType': 'String',
                    'StringValue': ddrequest['guid'],
                },
                'organization': {
                    'DataType': 'String',
                    'StringValue': d['organization']
                },
                'connector': {
                    'DataType': 'String',
                    'StringValue': d['connector']
                },
                'integration': {
                    'DataType': 'String',
                    'StringValue': d['integration']
                },
                'date': {
                    'DataType': 'String',
                    'StringValue': datetime.now().isoformat()
                },
                's3key': {
                    'DataType': 'String',
                    'StringValue': s3key
                },
                'body_embedded': {
                    'DataType': 'String',
                    'StringValue': str(embed_body)
                }
            },
            MessageBody=(sqsbody)
        )
```

- Now, here that the `s3Key` 


(So it can be reprocessed after.)


```
"s3_key": s3_key,



```


Pseudocode:

```
- on timestlot splitting.

- Precalculated the datetime and datetime local
curr_time = Utils.parseDate(eventData.timestamp)


Get the minute timestamp: e.g. 13:23:11:00:00
Get the minute Local Timestamp e.g. 07:23:11:00:00


```


Should it include the specific minute if the timestamp starts at x minute?

Am I creating new reprocessing queries?

```python
  

class JobService():

    """
    """

  

    def __init__(self,  

                 username: str, password: str, host: str, database: str,

                 organization_guid: str, connector_guid: str, processing_guid: str,

                 integration_name: str, job_parameters: dict, cloudwatchlog_message_function=None):

        # Key Parameters: postgresql credentials

        self.connection = psycopg2.connect(

            user=username,

            password=password,

            host=host,

            database=database

        )

  ...

        self.processing_guid = processing_guid
```

> Here we have the `processing_guid` and 

```python
  

processing_guid = job_parameters['guid']

organization_guid = job_parameters['organization']

connector_guid = job_parameters['connector']

integration_version = job_parameters['version']



event_datetime = job_parameters['date']

event_date = Utils.parseDate(event_datetime).date()



# example: s3://dev-stage-in/0238bfa6-2a89-42fe-8a67-fde4c80391ea/SFFLOW/4c374db5-8e37-428e-b988-92ce9879165b/2024-02-27/74da2bbe-1892-4d67-bd69-0ff76b55affc.json

S3_BUCKET = 'dev-stage-in'

# s3_key = f"s3://{S3_ROOT}/{organization_guid}/{job_integration}/{connector_guid}/{event_date}/{processing_guid}.json"

print('====================   JOB PARAMETER  ==========================')

pprint.pprint(job_parameters)

s3_key = f"{job_parameters['s3key']}"

processing_guid = job_parameters['processing_guid']

reprocess = False

if processing_guid is None:

	reprocess = True
```

```python
def defineTimeslot(self, curr_time: datetime.datetime, eventData: EventData) -> Timeslot:
		timestamp_datetime: datetime.datetime = curr_time
        timestamp_local_datetime: datetime.datetime = timestamp_datetime.astimezone(pytz.timezone(eventData.local_timezone))
        minute_timestamp_utc, minute_timestamp_local = 

```

- Reviewed Timeslot

```python
  

class Timeslot(StructureModel):

    """Timeslot

    This is the timeslot based event in which an hour is divided into 6 slots of 10 minutes.

    """

    def __init__(self,
                 event_guid: str,
                 organization_guid: str,
                 hour: int, minute: int, day: int,
                 month: int, year: int, week: int, weekday: int,
                 hour_local: int, minute_local: int, day_local: int, month_local: int,
                 year_local: int, week_local: int, weekday_local: int,
                 mouse_clicks: int,
                 keystrokes: int,
                 processing_guid: str,
                 ts1: int, ts5: int, ts10: int, ts15: int,
                 tl1: int, tl5: int, tl10: int, tl15: int,
                 event_end_time: datetime, # Used for timeslot creation, not inserted on table.
                 timestamp_utc: datetime, # Used for timeslot creation, not inserted on table.
                 timestamp_local: datetime # Used for timeslot creation, not inserted on table.

                 ):

        # guid will be generated by the postgresql server upon insertion

        self.event_guid = event_guid
        self.organization_guid = organization_guid
        self.hour = hour
        self.minute = minute
        self.day = day
        self.month = month
        self.year = year
        self.week = week
        self.weekday = weekday
        self.hour_local = hour_local
        self.minute_local = minute_local
        self.day_local = day_local
        self.month_local = month_local
        self.year_local = year_local
        self.week_local = week_local
        self.weekday_local = weekday_local
        self.mouse_clicks = mouse_clicks
        self.keystrokes = keystrokes
        self.processing_guid = processing_guid
        self.ts1 = ts1
        self.ts5 = ts5
        self.ts10 = ts10
        self.ts15 = ts15

        self.tl1 = tl1
        self.tl5 = tl5
        self.tl10 = tl10
        self.tl15 = tl15
        self.event_end_time = event_end_time
        self.timestamp = timestamp_utc
        self.timestamp_local = timestamp_local
```

> Lets re-deploy in 


![](img/Pasted%20image%2020240509102950.png)

- User note found for that connector what why??
	- It looks like it is running just fine here. 
![](img/Pasted%20image%2020240509103305.png)

- [ ] Maybe the developer database is incorrect.
![](img/Pasted%20image%2020240509103618.png)
> I am also correctly being monitored.

-  Surprisingly it seems that it was because of a bad env (there are two envs available)

Add the timestamp minute.

- Check the events/timeslots that are in the end.


Working on the prototype for the search of them. 


- Why not having to recalculate the time makes a difference?
- Because then you can recalculate the ts1, ts5, hour, ts10, ts15 using a modulo + and a delta. 


This seems to be stopping the insertions:

```
if eventData.action_type != 'ACTIVITY' or eventData.app != "" or eventData.app is not None:
```

![](img/Pasted%20image%2020240509115954.png)
> After moving the application down.

It still skipping the jobs, which makes me wonder:

How about eventData.app

I see poor logic:

```
if eventData.action_type != ACTIVITY or eventData.app == "" or eventData.app is None:
```

Here instead. Corrected.
![](img/Pasted%20image%2020240509120925.png)

Alright, now is not inserting the right timestamp. (all seem to be utc)

```
  

    def createMinuteTimestamp(self, timestamp_utc, timestamp_local)-> tuple[datetime.datetime, datetime.datetime]:

        """

        Creates the minute timestamp for the UTC and Local time.

        """

        timestamp_utc = timestamp_utc.replace(second=0, microsecond=0)

        timestamp_local.replace(tzinfo=pytz.utc)

        timestamp_local = timestamp_local.replace(second=0, microsecond=0)

        return timestamp_utc, timestamp_local
```

So how should I change the timestamp? How was it done before?

```python
timestamp_datetime: datetime.datetime = curr_time

timestamp_local_datetime: datetime.datetime = timestamp_datetime.astimezone(pytz.timezone(eventData.local_timezone))

timestamp_local_datetime = timestamp_local_datetime.replace(tzinfo=None)
```


```python
timestamp = Utils.parseDate(eventData.timestamp)

local_timestamp = timestamp.astimezone(timezone(eventData.local_timezone))

eventData.timestamp_local = local_timestamp.isoformat()

eventData.end_time_local = eventData.end_time_local.isoformat()
```


We know here is that the `total_rows` should not increase from `2955`


Now lets see
> Good it didn't increase the `processing_tracker`


![](img/Pasted%20image%2020240509161411.png)


The events updated-events should reflect near: 8:18 PM

![](img/Pasted%20image%2020240509161853.png)

![](img/Pasted%20image%2020240509162818.png)

- Collapsed Events 95
- Publishing timeslots 0 events 95











### Researching About Storing Email Databases

- Snowflake
- What kind of analytics might be useful?
- Considerations
	- Companies Datasets could change
	- It might be useful
- To ask?
	- How data comes in?
		- Hook
	- How data comes out should support?


How about simulating a response ratio to find also which are the customers we should 

Good, it looks good, lets try now with adding all the other stuff.

- [x] Lets add now the proper columns to staging, production, testing, etc.
- [x] Create the make file changes as well

It seems that I have the same issue here:

![](img/Pasted%20image%2020240509125824.png)

When you check the .env of the docker seems fine:

![](img/Pasted%20image%2020240509125901.png)

Here is the same error as before with the wrong env requirements

```sql
SELECT c.user_id, u.timezone from connector AS C JOIN users AS u ON u.id=c.user_id WHERE c.guid='4c374db5-8e37-428e-b988-92ce9879CHRO'
```

![](img/Pasted%20image%2020240509125928.png)
> Here you can see that uses the right connector to fetch.



This is the query that fails:
```
SELECT c.user_id, u.timezone from connector AS C JOIN users AS u ON u.id=c.user_id WHERE c.guid='4c374db5-8e37-428e-b988-92ce9879CHRO'  AND is_monitored = TRUE;
```

![](img/Pasted%20image%2020240509132147.png)

> Interesting so it does load correctly the last version


![](img/Pasted%20image%2020240509132407.png)

> Look, it even loads the correct workflow on processing. (When inside of the dockr exec)

While having this nonsense here?

![](img/Pasted%20image%2020240509132513.png)


Lets try removing the makefile:
![](img/Pasted%20image%2020240509132542.png)

Does it even run like this?


Absolutely no idea. I it's working just fine at the moment. 



## 10 


### Predicts.


Lets add the following predictions:

1. Adding the 

This breaks all predictions

```
        for product_id in self.product_ids:

            pred = self.model.predict(product_id, product_id)

            if (abs(pred.est - skip_recommend) <= .5):

                continue

            recommendation_list.append((self.id_to_products[product_id], pred.est))

        recommendation_list.sort(key=lambda x:x[1], reverse=True)
```


I wonder if `est` can have a confidence

Otherwise I can do the following:

- Find someone who bough that book.
- Add that again to the rows of all those transactions. with that user.

Keeps giving the same:

![](img/Pasted%20image%2020240510155707.png)

Ok, how about if we compare the baseline, and if the score is different than baseline only then add it.


No. I have a better one:

- Get a suggestion baseline. of similar books. Then rate the similar books?







